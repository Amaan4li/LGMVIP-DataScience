{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kWEWkjP9Ptqk"
   },
   "source": [
    "# LGM Virtual Internship Program"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Level Task 3 : Handwritten equation solver using CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Level Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset : https://www.kaggle.com/xainano/handwrittenmathsymbols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Name : Amaan Ali"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ----------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1WmfmRq3WMOc"
   },
   "outputs": [],
   "source": [
    "from skimage.io import imread\n",
    "from skimage.filters import gaussian, threshold_minimum\n",
    "from skimage.morphology import square, erosion, thin\n",
    "from skimage import img_as_ubyte\n",
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8VtYSoiMW6CO"
   },
   "outputs": [],
   "source": [
    "def binarize(image_abs_path):\n",
    "\n",
    "    # Convert color image (3-channel deep) into grayscale (1-channel deep)\n",
    "    # We reduce image dimensionality in order to remove unrelevant features like color.\n",
    "    grayscale_img = imread(image_abs_path, as_gray=True)\n",
    "\n",
    "    # Apply Gaussian Blur effect - this removes image noise\n",
    "    gaussian_blur = gaussian(grayscale_img)\n",
    "\n",
    "    # Apply minimum threshold\n",
    "    thresh_sauvola = threshold_minimum(gaussian_blur)\n",
    "\n",
    "    # Convert thresh_sauvola array values to either 1 or 0 (white or black)\n",
    "    binary_img = gaussian_blur > thresh_sauvola\n",
    "\n",
    "    return binary_img\n",
    "\n",
    "def shift(contour):\n",
    "\n",
    "    # Get minimal X and Y coordinates\n",
    "    x_min, y_min = contour.min(axis=0)[0]\n",
    "\n",
    "    # Subtract (x_min, y_min) from every contour point\n",
    "    return np.subtract(contour, [x_min, y_min])\n",
    "\n",
    "def get_scale(cont_width, cont_height, box_size):\n",
    "\n",
    "    ratio = cont_width / cont_height\n",
    "\n",
    "    if ratio < 1.0:\n",
    "        return box_size / cont_height\n",
    "    else:\n",
    "        return box_size / cont_width\n",
    "\n",
    "def extract_patterns(image_abs_path):\n",
    "\n",
    "    max_intensity = 1\n",
    "    # Here we define the size of the square box that will contain a single pattern\n",
    "    box_size = 28\n",
    "\n",
    "    binary_img = binarize(image_abs_path)\n",
    "\n",
    "    # Apply erosion step - make patterns thicker\n",
    "    eroded_img = erosion(binary_img, selem=square(3))\n",
    "\n",
    "    # Inverse colors: black --> white | white --> black\n",
    "    binary_inv_img = max_intensity - eroded_img\n",
    "\n",
    "    # Apply thinning algorithm\n",
    "    thinned_img = thin(binary_inv_img)\n",
    "\n",
    "    # Before we apply opencv method, we need to convert scikit image to opencv image\n",
    "    thinned_img_cv = img_as_ubyte(thinned_img)\n",
    "\n",
    "    # Find contours\n",
    "    contours, _ = cv2.findContours(thinned_img_cv, mode=cv2.RETR_EXTERNAL, method=cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Sort contours from left to right (sort by bounding rectangle's X coordinate)\n",
    "    contours = sorted(contours, key=lambda cont: cv2.boundingRect(cont)[0])\n",
    "\n",
    "     # Initialize patterns array\n",
    "    patterns = []\n",
    "\n",
    "    for contour in contours:\n",
    "\n",
    "        # Initialize blank white box that will contain a single pattern\n",
    "        pattern = np.ones(shape=(box_size, box_size), dtype=np.uint8) * 255\n",
    "\n",
    "        # Shift contour coordinates so that they are now relative to its square image\n",
    "        shifted_cont = shift(contour)\n",
    "\n",
    "        # Get size of the contour\n",
    "        cont_width, cont_height = cv2.boundingRect(contour)[2:]\n",
    "        # boundingRect method returns width and height values that are too big by 1 pixel\n",
    "        cont_width -= 1\n",
    "        cont_height -= 1\n",
    "\n",
    "        # Get scale - we will use this scale to interpolate contour so that it fits into\n",
    "        # box_size X box_size square box.\n",
    "        scale = get_scale(cont_width, cont_height, box_size)\n",
    "\n",
    "        # Interpolate contour and round coordinate values to int type\n",
    "        rescaled_cont = np.floor(shifted_cont * scale).astype(dtype=np.int32)\n",
    "\n",
    "        # Get size of the rescaled contour\n",
    "        rescaled_cont_width, rescaled_cont_height = cont_width * scale, cont_height * scale\n",
    "\n",
    "        # Get margin\n",
    "        margin_x = int((box_size - rescaled_cont_width) / 2)\n",
    "        margin_y = int((box_size - rescaled_cont_height) / 2)\n",
    "\n",
    "        # Center pattern wihin a square box - we move pattern right by a proper margin\n",
    "        centered_cont = np.add(rescaled_cont, [margin_x, margin_y])\n",
    "\n",
    "        # Draw centered contour on a blank square box\n",
    "        cv2.drawContours(pattern, [centered_cont], contourIdx=0, color=(0))\n",
    "\n",
    "        patterns.append(pattern)\n",
    "\n",
    "    return patterns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HR4DlCCDXUiE"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "from IPython.display import Image, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L9LU3b8xa3PW",
    "outputId": "ec18753a-175d-4928-fdf7-86e625ef522a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "# Load the Drive helper and mount\n",
    "from google.colab import drive\n",
    "\n",
    "# This will prompt for authorization.\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nCBRHlrQdKY3",
    "outputId": "2944f8eb-1b1a-48d7-97f3-48f7617b17b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "from zipfile import ZipFile\n",
    "file_name = \"my_model.zip\"\n",
    "\n",
    "with ZipFile(file_name, 'r') as zip:\n",
    "  zip.extractall()\n",
    "  print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BdPJeqf8eWrk",
    "outputId": "c9e359f9-dd20-4290-87be-e52c178ab071"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.load_model('/content/my_model')\n",
    "class_names = ['%', '*', '+', '-', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '[', ']']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q4wQ6e_6ejho",
    "outputId": "c71c07e2-55c7-4bab-94b2-56fc8bf86ea2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "rescaling_2 (Rescaling)      (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 28, 28, 16)        160       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 14, 14, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 14, 14, 32)        4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 7, 7, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 7, 7, 64)          18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 3, 3, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 3, 3, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 576)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               73856     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                2064      \n",
      "=================================================================\n",
      "Total params: 99,216\n",
      "Trainable params: 99,216\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 117
    },
    "id": "RdYHWjEPdKjj",
    "outputId": "d0a91427-77df-46a5-acb9-f11657f59816"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAUElEQVR4nMWPwQoAIAhDN+n/f9luaRpCUbSbe+qUikn0tTifCDKY0DQJjT2CQlegVjB3bGaygudrfcI4z7KGJdFS901DHrhw0AKygo8y/8IOmW8LNh8f7EsAAAAASUVORK5CYII=",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras_preprocessing/image/utils.py:107: UserWarning: grayscale is deprecated. Please use color_mode = \"grayscale\"\n",
      "  warnings.warn('grayscale is deprecated. Please use '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to 4 with a 100.00 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "img_path = \"/content/4.png\"\n",
    "display(Image(filename=img_path)) \n",
    "img = keras.preprocessing.image.load_img(\n",
    "    img_path, target_size=(28, 28), grayscale=True\n",
    ")\n",
    "img_array = keras.preprocessing.image.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, 0)\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5MOIqfrtewW8"
   },
   "outputs": [],
   "source": [
    "def predict_image(img):\n",
    "    img_array = keras.preprocessing.image.img_to_array(img)\n",
    "    img_array = tf.expand_dims(img_array, 0) # Create a batch\n",
    "    predictions = model.predict(img_array)\n",
    "    score = tf.nn.softmax(predictions[0])\n",
    "    \n",
    "    return (class_names[np.argmax(score)], 100 * np.max(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 259
    },
    "id": "rtvsZOQTe3_7",
    "outputId": "4917337c-b7cd-4bfd-eabb-2772bedb1659"
   },
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('9', 98.79574179649353)\n",
      "('+', 99.32170510292053)\n",
      "('3', 99.99513626098633)\n",
      "('-', 99.91315007209778)\n",
      "('5', 99.1050124168396)\n",
      "Equation: 9 + 3 - 5 \n",
      "Solution: 7\n"
     ]
    }
   ],
   "source": [
    "img_path = \"/content/test1.jpeg\"\n",
    "display(Image(filename=img_path)) \n",
    "patterns = extract_patterns(img_path)\n",
    "eq = ''\n",
    "\n",
    "for pattern in patterns:\n",
    "    patrern_string = predict_image(pattern)\n",
    "    print(patrern_string)\n",
    "    eq = eq +  str(patrern_string[0]) + \" \"\n",
    "\n",
    "print(\"Equation:\", eq)\n",
    "print(\"Solution:\", eval(eq))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Task3_HandwrittenEquationSolverUsingCNN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
